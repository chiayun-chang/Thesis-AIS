{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import contextily as ctx\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "from geopandas import GeoDataFrame, read_file\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "from fiona.crs import from_epsg\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import datashader as ds\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from holoviews.operation.datashader import datashade, dynspread,  rasterize\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.set_mapbox_access_token('my token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AIS data from Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For opening local files\n",
    "import pathlib\n",
    "\n",
    "import uuid\n",
    "\n",
    "# Test connection\n",
    "# Make sure you have pip  install  azure-storage-blob==2.1.0 installed\n",
    "# Do not install 12.1. this if\n",
    "# not compatible yet with adlfs\n",
    "import azure.storage.blob\n",
    "from azure.storage.blob import BlockBlobService, PublicAccess\n",
    "\n",
    "# this module loads dataframes in parallel\n",
    "# requires pip install dask[complete] and fastparquet  and python-snappy\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# this is for environmental variables for secrets (needs python-dotenv)\n",
    "# You can copy the  .env.example file and rename it to .env (one directory  up from the notebooks)\n",
    "# \n",
    "%load_ext dotenv\n",
    "# Load environment variables from the .env file 1 directory up\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "%dotenv -v\n",
    "\n",
    "# This should print 2.1.0\n",
    "azure.storage.blob.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the environment variable from the  .env file\n",
    "sas_token = os.environ['AZURE_BLOB_SAS_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the 2.1 version.  if the  BlockBlobService is missing, you probably installed a later version\n",
    "service = azure.storage.blob.BlockBlobService(sas_token=sas_token, account_name='rwsais')\n",
    "# As a test, show the  first blob\n",
    "blob = next(iter(service.list_blobs('ais',  prefix='parquet')))\n",
    "# this is one of subfiles that rijkswaterstaat provided\n",
    "blob.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the blobs inside the container\n",
    "print(\"\\nList blobs in the container\")\n",
    "generator = blob_service.list_blobs('chia-yun-results')\n",
    "for blob in generator:\n",
    "    print(\"\\t Blob name: \" + blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads October 2016 data into memory (000 to 176)\n",
    "files = []\n",
    "for i in range(0,176):\n",
    "    files.append(f'abfs://ais/parquet/201610_201710_nijmegen/x00{str(i).zfill(3)}_201610_201710_nijmegen.parquet')\n",
    "df = dd.read_parquet(files, storage_options={'account_name': 'rwsais', 'sas_token': sas_token})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads December 2016 data into memory (000 to 155)\n",
    "files = []\n",
    "for i in range(156):\n",
    "    files.append(f'abfs://ais/parquet/201612_201712_nijmegen/x00{str(i).zfill(3)}_201612_201712_nijmegen.parquet')\n",
    "df = dd.read_parquet(files, storage_options={'account_name': 'rwsais', 'sas_token': sas_token})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads October 2017 data into memory (175 to 324)\n",
    "files = []\n",
    "for i in range(175,324):\n",
    "    files.append(f'abfs://ais/parquet/201610_201710_nijmegen/x00{i}_201610_201710_nijmegen.parquet')\n",
    "df = dd.read_parquet(files, storage_options={'account_name': 'rwsais', 'sas_token': sas_token})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads December 2017 data into memory (156 to 306)\n",
    "files = []\n",
    "for i in range(155,306):\n",
    "    files.append(f'abfs://ais/parquet/201612_201712_nijmegen/x00{i}_201612_201712_nijmegen.parquet')\n",
    "df = dd.read_parquet(files, storage_options={'account_name': 'rwsais', 'sas_token': sas_token})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(f'abfs://chia-yun-results/waal_201610.parquet', \n",
    "                     storage_options={'account_name': 'rwsais', 'sas_token': sas_token})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove other year's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 2017 rows in 2016 dataframe\n",
    "df = df[df.timestamplast.dt.year == 2016]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 2016 rows in 2017 dataframe\n",
    "df = df[df.timestamplast.dt.year == 2017]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove speed over ground(sog) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016\n",
    "print(\"Original size: {} rows\".format(len(df)))\n",
    "zero = df[df.sog == 0]\n",
    "print(\"Reduced {} rows of speed 0\".format(len(zero)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove vesseltype between 0 and 19<br>\n",
    "0 Not available (default)<br>\n",
    "1-19 Reserved for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016\n",
    "print(\"Original size: {} rows\".format(len(df)))\n",
    "nan = df[df.vesseltype < 20]\n",
    "print(\"{} rows vesseltype is between 0 to 19\".format(len(nan)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['sog'] > 0]\n",
    "df = df[df['vesseltype'] > 19]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use datashade to plot all data points on map with little memory demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "points = hv.Points(df[['longitude','latitude']])\n",
    "# Create an overview of all points\n",
    "datashade(points).opts(height=300, width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of rows per unique ship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2016-12 mmsi 240100000 is an unusual ship that has 1 million records, and SOG is 0.1, and stay at one spot. From index 7167626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.index[7167626:],inplace=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = df.groupby(['new_id']).size()\n",
    "records = records.to_frame()\n",
    "records.rename( columns={0:'counts'}, inplace=True )\n",
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(records, x='counts', labels={'counts':'Number or records per unique ship'},\n",
    "                   range_x=(0,30000), title='2016-12')\n",
    "fig.show()\n",
    "#fig.write_html('records_per_ship_201612.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(records, y=\"counts\", labels={'counts':'Number or records per unique ship'}, \n",
    "                range_y=(0,30000) ,title='2016-12')\n",
    "fig.show()\n",
    "#fig.write_html('records_per_ship_201612_violin.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(records, x='counts', labels={'counts':'Number or records per unique ship'}, \n",
    "                   range_x=(0,30000), title='2017-12')\n",
    "fig.show()\n",
    "#fig.write_html('records_per_ship_201712.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(records, y=\"counts\", labels={'counts':'Number or records per unique ship'}, \n",
    "                range_y=(0,30000) ,title='2017-12')\n",
    "fig.show()\n",
    "#fig.write_html('records_per_ship_201712_violin.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x201610 = records\n",
    "#x201612 = records\n",
    "#x201710 = records\n",
    "x201712 = records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ooverlay all 4 periods\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=x201712.counts, name='2017-12', \n",
    "                           ))\n",
    "fig.add_trace(go.Histogram(x=x201612.counts, name='2016-12', \n",
    "                           ))\n",
    "fig.add_trace(go.Histogram(x=x201710.counts, name='2017-10', \n",
    "                           ))\n",
    "fig.add_trace(go.Histogram(x=x201610.counts, name='2016-10',\n",
    "                           marker_color='#FFD700'))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(#barmode='overlay', \n",
    "                  xaxis_title_text='Number or records per unique ship', # xaxis label\n",
    "                  yaxis_title_text='Counts' # yaxis label\n",
    "                 )\n",
    "fig.show()\n",
    "#fig.write_html('records_per_ship_all4_outliers.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count how many ships have missing information\n",
    "2016 cargo & tanker: 5063\n",
    "2017 cargo & tanker: 5730 \n",
    "\"\"\"\n",
    "df[df.eq(0).any(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select points on the Waal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['timestamplast']=='2016-12-20 18:19:56+00:00'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2016 dataset needs to remove last 1.6 million duplicates\n",
    "and remove SOG < 0 because they are basically not moving so records intervals are around 4 mins\n",
    "\"\"\"\n",
    "df.drop(df.index[7167626:],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoDataFrame\n",
    "gdf = GeoDataFrame(df, crs = 'EPSG:4326', geometry = gpd.points_from_xy(df.longitude, df.latitude))\n",
    "gdf.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Waal\n",
    "polygon = gpd.read_file('waal.geojson')\n",
    "\n",
    "# Clip the points inside polygon\n",
    "df = gpd.clip(gdf,polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t'] = pd.to_datetime(df['timestamplast'], format='%Y-%m-%d %H:%M:%S').dt.tz_localize(None)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(columns='geometry', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign new ID to vessels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many not-unique unique IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='mmsi',inplace=True)\n",
    "df.drop_duplicates(subset=['callsign','vesseltype','mmsi','length','width'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = df[(df['length'].isna()) & (df['width'].isna())]\n",
    "na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_unique = df.sort_values(by='mmsi')\n",
    "not_unique = not_unique.groupby(by='mmsi')['eni'].count().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_unique = not_unique[not_unique['eni']>1]\n",
    "not_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign ID to unique vessel\n",
    "df = df.assign(new_id=(df['mmsi'].astype(str) + '_' + df['width'].astype(str) + '_' + df['length'].astype(str) + '_' + df['vesseltype'].astype(str)).astype('category').cat.codes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='geometry', inplace=True)\n",
    "df.reset_index(drop=False,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mmsi'].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
